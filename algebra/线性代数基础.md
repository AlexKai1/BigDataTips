##线性代数基础
###标量、向量、矩阵和张量

* 标量：一个标量就是一个单独的数，一般用小写的变量名称表示。当然，当我们介绍标量时，要明确它们是哪种类型的数值。这个在写论文时要注意，比如：在定义自然数标量时，我们可能会说”令n ∈ N表示元素的数目”。
* 向量：在物理学和工程学中，几何向量更常被称为矢量，这个学过高中数学和物理的就知道，但在线性代数中，经过进一步的抽象，大小和方向的概念亦不一定适用，但我们可以简单的理解为一列数，通过这列数中的索引，我们可以确定每个单独的数。通常会赋予向量粗体的小写名称。当我们需要明确表示向量中的元素时，我们会将元素排列成一个方括号包围的纵柱
* 矩阵：矩阵是二维数组，其中的每一个元素被两个索引而非一个所确定。我们通常会赋予矩阵粗体的大写变量名称，比如A。 如果一个实数矩阵高度为m，宽度为n，那么我们说A\epsilon R^{m\times n} ，当我们到明确表达矩阵的时候，我们将它们写在用方括号包围起来的数组中
* 张量：线性代数或几何代数中定义的张量是基于向量和矩阵的推广，通俗一点理解的话，我们可以将标量视为零阶张量，向量（矢量）视为一阶张量，那么矩阵就是二阶张量。例如，可以将任意一张彩色图片表示成一个三阶张量（就像C语言中的三维数组），三个维度分别是图片的高度、宽度和色彩数据。 使用字体 A 来表示张量 “A’’。张量 A 中坐标为 (i, j, k) 的元素记作 $A_{i,j,k}$

###矩阵向量的运算
* 矩阵乘法：是矩阵运算中最重要的操作之一。两个矩阵 A 和 B 的 矩阵乘积(matrix product)是第三个矩阵 C。为了使乘法定义良好,矩阵 A 的列数必须和矩阵 B 的行数相等。如果矩阵 A 的形状是 m × n,矩阵 B 的形状是 n × p,那么矩阵C 的形状是 m × p。我们可以通过将两个或多个矩阵并列放置以书写矩阵乘法,例如C = AB
    矩阵乘积服从分配律:A(B + C) = AB + AC

    矩阵乘积也服从结合律：A(BC) = (AB)C

    但不同于标量乘积,矩阵乘积并不满足交换律(AB = BA 的情况并非总是满足)。
* 矩阵转置
    * 矩阵转置的结果为 $a_{i,j}=(a_{i,j})^{T}$
    * $R\bullet R^{T}$ 结果为对称矩阵，由 $R\bullet R^{T}=(R\bullet R^{T})^{T}$ ,得证 $R\bullet R^{T}$ 结果为对称矩阵

###单位矩阵和逆矩阵
线性代数提供了被称为矩阵逆的强大工具。 对于大多数矩阵A，我们都能通过矩阵逆解析地求解。

为了描述矩阵逆，我们首先需要定义单位矩阵的概念。 任意向量和单位矩阵相乘，都不会改变。 我们将保持 n 维向量不变的单位矩阵记作 $I_{n}$ 。 形式上 $I_{n}\in R^{n}$

$\forall x\in R^{n},I_{n}x=x$ 。

单位矩阵的结构很简单：所有沿主对角线的元素都是1，而所有其他位置的元素都是0。 
![](https://pic4.zhimg.com/80/v2-76f5c3a9d7d10fea474c58cd308bdae5_hd.jpg)
矩阵A的矩阵逆记作 $A^{-1}$ ，其定义的矩阵满足如下条件
$A^{-1}A=I_{n}$
如何求逆矩阵：
[初等行运算求逆矩阵](http://www.shuxuele.com/algebra/matrix-inverse-row-operations-gauss-jordan.html)

###方差，标准差，协方差
* 方差：是衡量随机变量或一组数据时离散程度的度量，方差计算公式：
    ![](https://pic4.zhimg.com/80/v2-1abb9c1049d755992a77c15f8acb96db_hd.jpg)   
    其中 $\sigma^{2}$ 为总体方差， X 为变量， $\mu$ 为总体均值， N 为总体例数。下面的标准差公式中亦相同。
* 标准差：也被称为标准偏差，或者实验标准差，公式为
![](https://pic2.zhimg.com/80/v2-e716c62e213b641b836667438a7de4c3_hd.jpg)  
标准差是方差的算术平方根。标准差能反映一个数据集的离散程度。平均数相同的两组数据，标准差未必相同。
为什么需要协方差？

我们知道，标准差和方差一般是用来描述一维数据的，但现实生活我们常常遇到含有多维数据的数据集，最简单的 大家上学时免不了要统计多个学科的考试成绩。面对这样的数据集，我们当然可以按照每一维独立的计算其方差，但是通常我们还想了解更多，比如，一个男孩子的 猥琐程度跟他受女孩子欢迎程度是否存在一些联系。协方差就是这样一种用来度量两个随机变量关系的统计量。
数学上可以用两个字段的协方差表示其相关性，由于已经让每个字段均值为0，则：
$$Cov(a,b)=\frac{1}{m}\sum_{i=1}^m{a_ib_i}$$

协方差矩阵

理解协方差矩阵的关键就在于牢记它计算的是不同维度之间的协方差，而不是不同样本之间，拿到一个样本矩阵，我们最先要明确的就是一行是一个样本还是一个维度，心中明确这个整个计算过程就会顺流而下，这么一来就不会迷茫了
协方差矩阵的计算：
将每个字段内所有值都减去字段均值，其结果是将每个字段都变为均值为0.变换后的矩阵，
假设我们只有a和b两个字段，那么我们将它们按行组成矩阵X：
$$X=\begin{pmatrix}
  a_1 & a_2 & \cdots & a_m \\
  b_1 & b_2 & \cdots & b_m
\end{pmatrix}$$
然后我们用X乘以X的转置，并乘上系数1/m：
$$\frac{1}{m}XX^\mathsf{T}=\begin{pmatrix}
  \frac{1}{m}\sum_{i=1}^m{a_i^2}   & \frac{1}{m}\sum_{i=1}^m{a_ib_i} \\
  \frac{1}{m}\sum_{i=1}^m{a_ib_i} & \frac{1}{m}\sum_{i=1}^m{b_i^2}
\end{pmatrix}
$$
这个矩阵对角线上的两个元素分别是两个字段的方差，而其它元素是a和b的协方差。两者被统一到了一个矩阵的。

根据矩阵相乘的运算法则，这个结论很容易被推广到一般情况：

设我们有m个n维数据记录，将其按列排成n乘m的矩阵X，设$C=\frac{1}{m}XX^\mathsf{T}$，则C是一个对称矩阵，其对角线分别个各个字段的方差，而第i行j列和j行i列元素相同，表示i和j两个字段的协方差。

###范数
范数其实就是衡量一个向量大小的单位。在机器学习中，我们也经常使用被称为范数(norm) 的函数衡量矩阵大小.
常见的：

$L^{1} 范数\left| \left| x \right| \right| $：为x向量各个元素绝对值之和；

$L^{2} 范数\left| \left| x \right| \right| _{2} $：为x向量各个元素平方和的开方，这个也就是两点直线距离嘛，回忆初高中的知识！

注意：当 p = 2 时, $L^{2}$ 范数被称为 欧几里得范数(Euclidean norm)。它表示从原点出发到向量 x 确定的点的欧几里得距离。 $L^{2} $ 范数在机器学习中出现地十分频繁经常简化表示为 $∥x∥$,略去了下标 2。平方 $L^{2}$ 范数也经常用来衡量向量的大小,可以简单地通过点积 $x^{T}x$ 计算。
###特殊类型的矩阵和向量
* 对角矩阵(diagonal matrix)：只在主对角线上含有非零元素,其他位置都是零。形式上,矩阵 是对角矩阵,当且仅当对于所有的 i=j，$D_{i,j}$不等于0
* 单位矩阵是对角元素全部是 1的对角矩阵。
* 单位向量：指模等于1（具有 单位范数）的向量。由于是非零向量，单位向量具有确定的方向。单位向量有无数个。也就是说：对于单位向量，有 $||x||_{2} = 1$.
* 对称矩阵:是转置和自己相等的矩阵： $A=A^{T}$
* 正交矩阵:是指行向量和列向量是分别标准正交的方阵： $A^{T}A=AA^{T}=I$  
  这意味着 $A^{-1}=A^{T}$
  

### 特征分解以及其意义

许多数学对象可以通过将它们分解成多个组成部分，或者找到它们的一些属性而更好地理解，这些属性是通用的，而不是由我们选择表示它们的方式引起的。

例如:整数可以分解为质数。 我们可以用十进制或二进制等不同方式表示整数12，但质因数分解永远是对的12=2×3×3。 从这个表示中我们可以获得一些有用的信息，比如12不能被5整除，或者12的倍数可以被3整除。

正如我们可以通过分解质因数来发现整数的一些内在性质，我们也可以通过分解矩阵来发现矩阵表示成数组元素时不明显的函数性质。

* 特征分解是使用最广的矩阵分解之一，即我们将矩阵分解成一组特征向量和特征值。
* 一个变换（或者说矩阵）的特征向量就是这样一种向量，它经过这种特定的变换后保持方向不变，只是进行长度上的伸缩而已。

特征向量的原始定义： $AX=CX$
可以很容易看出， CX 是方阵 A 对向量 X  进行变换后的结果，显然 CX 和 X 的方向相同。 X 是特征向量的话， C表示的就是特征值。
求解：令 A 是一个 N×N 的方阵，且有 N 个线性无关的特征向量 $q_{i}(i=1....N)$
这样， A 可以被分解
$$A = Q \Lambda Q^{-1}$$
其中 Q 是N×N方阵，且其第 i列为 A 的特征向量 。 Λ 是对角矩阵，其对角线上的元素为对应的特征值，也即 $\wedge _{ii}=C_{i}$ 

特征值及特征向量的几何意义和物理意义：

在空间中，对一个变换而言，特征向量指明的方向才是很重要的，特征值不那么重要。虽然我们求这两个量时先求出特征值，但特征向量才是更本质的东西！特征向量是指经过指定变换（与特定矩阵相乘）后不发生方向改变的那些向量，特征值是指在经过这些变换后特征向量的伸缩的倍数,也就是说矩阵对某一个向量或某些向量只发生伸缩变换，不对这些向量产生旋转的效果，那么这些向量就称为这个矩阵的特征向量，伸缩的比例就是特征值。

物理的含义就是图像的运动：特征向量在一个矩阵的作用下作伸缩运动，伸缩的幅度由特征值确定。特征值大于1，所有属于此特征值的特征向量身形暴长；特征值大于0小于1，特征向量身形猛缩；特征值小于0，特征向量缩过了界，反方向到0点那边去了。
特征分解的重要应用:
PCA
[PCA以及数据基础](http://blog.codinglabs.org/articles/pca-tutorial.html)

###奇异值分解及其意义
TB


