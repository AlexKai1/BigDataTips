##贝叶斯分类器
贝叶斯分类器是一种概率框架下的统计学习分类器，对分类任务而言，假设在相关概率都已知的情况下，贝叶斯分类器考虑如何基于这些概率为样本判定最优的类标。
![](http://img.blog.csdn.net/20170326215305995?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMTgyNjQwNA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

##贝叶斯决策论
若将上述定义中样本空间的划分Bi看做为类标，A看做为一个新的样本，则很容易将条件概率理解为样本A是类别Bi的概率。在机器学习训练模型的过程中，往往我们都试图去优化一个风险函数，因此在概率框架下我们也可以为贝叶斯定义“条件风险”（conditional risk）
![](http://img.blog.csdn.net/20170326215332621?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMTgyNjQwNA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
我们的任务就是寻找一个判定准则最小化所有样本的条件风险总和，因此就有了贝叶斯判定准则（Bayes decision rule）:为最小化总体风险，只需在每个样本上选择那个使得条件风险最小的类标。
![](http://img.blog.csdn.net/20170326215401012?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMTgyNjQwNA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
若损失函数λ取0-1损失，则有：
![](http://img.blog.csdn.net/20170326215415152?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMTgyNjQwNA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
即对于每个样本x，选择其后验概率P（c | x）最大所对应的类标，能使得总体风险函数最小，从而将原问题转化为估计后验概率P（c | x）。一般这里有两种策略来对后验概率进行估计：

>
* 判别式模型：直接对 P（c | x）进行建模求解。决策树、神经网络、SVM都是属于判别式模型。
* 生成式模型：通过先对联合分布P（x,c）建模，从而进一步求解 P（c | x）。

贝叶斯分类器就属于生成式模型:
![](http://img.blog.csdn.net/20170326215554592?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMTgyNjQwNA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
其中 ：

* P(c)是类的先验概率，prior
* P(x|c)是样本x相对于类标记c的 类条件概率，或者称为 似然 (likelihood)
* P(x)是用于归一化的证据因子
* P(x)与类标记无关，给定样本x的话(可以用全概率公式求的)

对于类先验概率P（c），p（c）就是样本空间中各类样本所占的比例，根据大数定理（当样本足够多时，频率趋于稳定等于其概率），这样当训练样本充足时，p(c)可以使用各类出现的频率来代替。因此只剩下类条件概率p（x | c ），它涉及关于x的所有属性的联合概率，直接根据样本出来的频率估计会遇到严重的困难。例如，假设样本的d个属性都是二值的，则样本空间将有$2^d$种可能的取值，再现实应用中，这个取值往往远大于训练样本数，也就是说，很多样本在训练集合中根本没有出现，直接使用概率来估计P(x|c)显然不可行，因为"未被观察到"与“出现概率为0”通常不同

##极大似然估计
极大似然估计（Maximum Likelihood Estimation，简称MLE），是一种根据数据采样来估计概率分布的经典方法。常用的策略是先假定总体具有某种确定的概率分布，再基于训练样本对概率分布的参数进行估计。运用到类条件概率p（x | c ）中，假设p（x | c ）服从一个参数为θ的分布，问题就变为根据已知的训练样本来估计θ。极大似然法的核心思想就是：估计出的参数使得已知样本出现的概率最大，即使得训练数据的似然最大。
![](http://img.blog.csdn.net/20170326215646390?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMTgyNjQwNA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
例如：假设样本属性都是连续值，p（x | c ）服从一个多维高斯分布 $p(x|c) $~$ N (\mu_c,\sigma_c^2)$，则通过MLE计算出的参数$\mu_c,\sigma_c^2$的极大似然估计为：
![](http://img.blog.csdn.net/20170326215746311?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMTgyNjQwNA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
也就是说通过极大似然法得到的正态分布均值就是样本均值。
需要注意的是，这种参数化的方法虽能使类条件概率估计变的相对简单，但是估计结果的准确率严重依赖于所假设的概率分布形式是否符合真实的数据分布。

###朴素贝叶斯分类器
不难看出：原始的贝叶斯分类器最大的问题在于联合概率密度函数的估计，首先需要根据经验来假设联合概率分布，其次当属性很多时，训练样本往往覆盖不够，参数的估计会出现很大的偏差。为了避免这个问题，朴素贝叶斯分类器（naive Bayes classifier）采用了“属性条件独立性假设”，即样本数据的所有属性之间相互独立。这样类条件概率p（x | c ）可以改写为：

$$P(C|x) = \frac{P(C)P(x|c)}{P(x)}==\frac{P(c)}{P(x)} \prod_{i=1}^d P(x_i|c)$$

其中，d为属性数目，$x_i$为x在第i个属性上的取值。
由于 P(x)对于所有类别相同，因此可以光计算分子上的。
显然，朴素贝叶斯分类的训练过程就是基于训练集合D来估计类先验概率P(c),并为每个属性估计条件概率$P(x_i|c)$
这样，为每个样本估计类条件概率变成为每个样本的每个属性估计类条件概率。
![](http://img.blog.csdn.net/20170326215824874?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMTgyNjQwNA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
对连续属性考虑概率密度函数，假定$p(x|c) $~$ N (\mu_c,\sigma_c^2)$,其中 $\mu_i$和$\sigma_{c,i}^2$分别是第c类样本在第i个属性上取值的均值和方差。
相比原始贝叶斯分类器，朴素贝叶斯分类器基于单个的属性计算类条件概率更加容易操作，需要注意的是：若某个属性值在训练集中和某个类别没有一起出现过，这样会抹掉其它的属性信息，因为该样本的类条件概率被计算为0。因此在估计概率值时，常常用进行平滑（smoothing）处理，拉普拉斯修正（Laplacian correction）就是其中的一种经典方法，具体计算方法如下：
![](http://img.blog.csdn.net/20170326215843299?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMTgyNjQwNA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)



  



