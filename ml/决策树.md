#决策树  

##概况
决策树(Decision tree),是分类(classification)中常用的算法。决策树是一种类似于流程图的树结构，其中，每个内部节点（非树叶节点）表示在一个属性上的测试，每个分支表示该测试的一个输出，而每个树叶节点（终端节点）存放一个类标号。

##算法过程  
决策树是通过自顶向下的分治方法构造（贪心非回溯），从训练集和他们相关联的类标号开始构造决策树，随着树的构造，训练集递归的划分为较小的子集。  

策略如下：

D:数据分区,开始时候是数据和相应类标号的完全集。  
attribute_list:数据属性的列表  
attribute_selection_method:指定选择属性的启发式过程，用来选择可以按类“最好的”区分给定数据的属性。  

 *  树从单个节点N开始，N代表D中的训练集。
 *  如果D中的数据都为一类，则节点N变成树叶，并用该类标记他。
 *  否则，算法调用attribute_selection_method 确定分裂准则(splitting criterion)。分裂准则通过确定把D中的数据划分成个体类的最好方法，告诉我们在节点N上对哪个属性进行测试，分类准则还告诉我们对于选定的测试，从节点N生长哪些分支。具体的说，分裂准则指定分裂属性，并且也指出分裂点或者分裂子集。分裂准则的确定原则：每个分支的输出分区尽可能的纯。
 * 对于D的每个结果分区，算法使用相同的过程递归地形成决策树。
 * 递归停止条件： 1没有剩余属性用来进一步划分，使用多数表决，决定分类。2给定的分支没有数据。
 * 返回结果。
 

## 属性选择度量
墒（ID3决策树使用），基尼系数（CART决策树使用）

## 树剪枝  
通常使用树剪枝的方法来处理过分拟合的情况。  

###先剪枝  
通过提前停止树的构造而对树进行剪枝，一旦停止，节点就成为树叶。在构造树时候，可以使用度量来评估划分的优劣，如果划分一个节点的数据导致低于预定义的划分，将停止给定子集的划分。

###后剪枝  
由完全生长的树减去子树。通过删除节点的分支并用树叶替换它而剪掉给定节点上的子树，该树叶的类标号用子树中最频繁的类标记。  

代价复杂度剪枝算法是后剪枝的一个实例，该方法把树的复杂度看做树中树叶节点的个数和树的错误率的函数（错误率：树误分类的数据所占的百分比），它从树的底开始，对于每个内部节点N，计算N的子树的代价复杂度和该子树剪枝后N的子树的代价复杂度。如果导致较小的代价复杂度，则剪枝。  

＝》按照西瓜书中所说，是否剪纸，也就是如何判断决策树泛化性能提升？   
可以使用留出法，也就是预留一部分数据用作validation以进行性能评估。

## 连续与缺失值    

###连续值处理  
连续属性离散化。最简单的策略是采用二分法对连续属性进行处理，C4.5决策树采用这种机制。  
另外，与离散属性不同，若当前节点划分属性为连续属性，该属性还可以作为其后代节点的划分属性。  

###缺失值处理  
我们需要解决两个问题:   

1.如何在属性值缺失的情况下进行划分属性选择？  
2.给定划分属性，若样本在该属性上缺失，如何对样本进行划分？  

假设：对属性a，p表示无缺失样本所占的比例, pk表示无缺失样本中第k类所占的比例，rv表示无缺失值样本在属性a上取值av的样本的比例。  
则Gain(D,a) = p * Gain(D',a)   

对于问题2，我们采取的方式是： 让同一个样本以不同的概率划分到要划分的字节点中去。 概率等于rv。参考西瓜树87-88页。
