## CART
###概念
CART,值得是Classification And Regression Tree的缩写。由于能够分类，也能回归，所以作为弱分类(回归)器，在RF,以及GBDT中得到广泛的使用。
假设我们有数据集D,X和Y分别是输入和输出变量，其中Y是连续变量(回归模型)，D可以表示为$D=\{(x^1,y^1),(x^2,y^2),...,(x^m,y^m) \} $,m为数据集的大小。
>一个回归树本质代表着一个被划分的输入空间和这些划分的输入空间上的各自取值。

比如，一个回归树将一个输出空间划分为K个部分，$R_1,R_2,...,R_K$,并且每个输入空间上都有一个固定的输出$C_K$,那么回归树模型可以表示为:
$$f(x) = \sum_{k=1}^K C_k I(x\in R_k)$$
也就是说，只要模型建立起来，只要知道输入向量属于哪个部分，就立刻能得到结果。I()是指示函数。  
CART如何划分空间呢？
切分变量：切分特征，就是在某个特征上面进行选择。
切分点：就是一个值，这个值将集合分为两边。
比如选择第j个特征$x_j$和一个切分点s，就能够定义两部分了：
$$R_1(j,s)=\{ x|x_j<=s \},R_2(j,s)=\{ x|x_j>s \}$$
划分空间后，那如何寻找最优切分特征以及最优切分点呢？
通过最小化损失函数：
$$\sum_{x\in R^k} (y^i-f(x^i)^2)$$
更具体一点，就是遍历所有输入变量来找最优切分特征以及最优切分点。
$$min_{j,s} [min_{c1}\sum_{x\in R_1(j,s)} (y^i-c_1)^2+min_{c2}\sum_{x\in R_2(j,s)} (y^i-c_2)^2]$$
通常认为一个区域的上面的结果为该区域对应的所有输出的均值：
$$\hat{c_m}=mean(y|x\in R_m)$$
也就是说，对于上面找最优切分点的时候，c1,c2可以替换为均值。

###算法细节
算法大致流程如下：
找到最优切分点（j,s）后，切分就能将集合划分为两个损失最少的部分。对于切分出来的区域再重复递归，直到满足条件为止。
1.遍历输入空间，找到最优切分特征以及切分点
$$min_{j,s} [min_{c1}\sum_{x\in R_1(j,s)} (y^i-c_1)^2+min_{c2}\sum_{x\in R_2(j,s)} (y^i-c_2)^2]$$
2.对于得到的最优切分特征以及切分点，划分区域，并得到输出值
$$R_1(j,s)=\{ x|x_j<=s \},R_2(j,s)=\{ x|x_j>s \}$$
$$\hat{c_m}=mean(y|x\in R_m)$$
3.对得到的两个子区域递归调用1，2步骤
4.输入空间划分为K个部分，$R_1,R_2,...,R_K$,并且每个输入空间上都有一个固定的输出$C_K$,那么回归树模型可以表示为:
$$f(x) = \sum_{k=1}^K C_k I(x\in R_k)$$
###参考
[博客1](http://blog.csdn.net/xierhacker/article/details/64439601)

