<h1>GBDT</h1>

<h2>介绍</h2>
Gradient Boost Decision Tree,是一个应用很广泛的算法,可以用来做分类、回归。最近在各种竞赛中取得了不错的效果，这个算法还有一些其他的名字，比如说MART(Multiple Additive Regression Tree)，GBRT(Gradient Boost Regression Tree)，Tree Net等，发明者是Friedman.   



<h2>Boosting Decision Tree：提升树算法</h2>
跟传统的Boost方法不同(比如AdaBoost),AdaBoost是为每个样本附上一个权重值，在每一步训练中，增加分类错误的样本的权重。再进行下一步迭代。最后将这些弱分类器组合起来，得到一个强分类器。  
而Boost Tree与传统的Boost的区别是，每一次的计算是为了减少上一次的残差(residual)，而为了消除残差，我们可以在残差减少的梯度(Gradient)方向上建立一个新的模型。所以说，在Boost中，每个新的模型的建立是为了使得之前模型的残差往梯度方向减少，与传统Boost对正确、错误的样本进行加权有着很大的区别。GB利用最速下降的近似方法，即利用损失函数的负梯度在当前模型的值，作为回归问题中提升树算法的残差的近似值，拟合一个回归树。（注：鄙人私以为，与其说负梯度作为残差的近似值，不如说残差是负梯度的一种特例）

假设对于一个样本x，它可能属于K个分类，其估计值分别为F1(x)…FK(x)，Logistic变换如下，logistic变换是一个平滑且将数据规范化（使得向量的长度为1）的过程，结果为属于类别k的概率pk(x)

![](http://images.cnblogs.com/cnblogs_com/LeftNotEasy/201103/201103072353109154.png)

对于Logistic变换后的结果，损失函数为：
![](http://images.cnblogs.com/cnblogs_com/LeftNotEasy/201103/201103072353101944.png)  

其中，yk为输入的样本数据的估计值，当一个样本x属于类别k时，yk = 1，否则yk = 0。将Logistic变换的式子带入损失函数，并且对其求导，可以得到损失函数的梯度：
![](http://images.cnblogs.com/cnblogs_com/LeftNotEasy/201103/201103072353105499.png)  
下面举个例子：

    假设输入数据x可能属于5个分类（分别为1,2,3,4,5），训练数据中，x属于类别3，则y = (0, 0, 1, 0, 0)，假设模型估计得到的F(x) = (0, 0.3, 0.6, 0, 0)，则经过Logistic变换后的数据p(x) = (0.16,0.21,0.29,0.16,0.16)，y - p得到梯度g：(-0.16, -0.21, 0.71, -0.16, -0.16)。观察这里可以得到一个比较有意思的结论：

    假设gk为样本当某一维（某一个分类）上的梯度:

    gk>0时，越大表示其在这一维上的概率p(x)越应该提高，比如说上面的第三维的概率为0.29，就应该提高，属于应该往“正确的方向”前进

                  越小表示这个估计越“准确”

    gk<0时，越小，负得越多表示在这一维上的概率应该降低，比如说第二维0.21就应该得到降低。属于应该朝着“错误的反方向”前进

                  越大，负得越少表示这个估计越“不错误 ”

    总的来说，对于一个样本，最理想的梯度是越接近0的梯度。所以，我们要能够让函数的估计值能够使得梯度往反方向移动（>0的维度上，往负方向移动，<0的维度上，往正方向移动）最终使得梯度尽量=0），并且该算法在会严重关注那些梯度比较大的样本，跟Boost的意思类似。
    
  得到梯度之后，就是如何让梯度减少了。这里是用的一个迭代+决策树的方法，当初始化的时候，随便给出一个估计函数F(x)（可以让F(x)是一个随机的值，也可以让F(x)=0），然后之后每迭代一步就根据当前每一个样本的梯度的情况，建立一棵决策树。就让函数往梯度的反方向前进，最终使得迭代N步后，梯度越小。
  这里建立的决策树和普通的决策树不太一样，首先，这个决策树是一个叶子节点数J固定的，当生成了J个节点后，就不再生成新的节点了。
  
  
 <h2>算法流程</h2>
 
 ![算法流程](http://images.cnblogs.com/cnblogs_com/LeftNotEasy/201103/20110307235312285.png)  
 
 0. 表示给定一个初始值

 1. 表示建立M棵决策树（迭代M次）

 2. 表示对函数估计值F(x)进行Logistic变换

 3. 表示对于K个分类进行下面的操作（其实这个for循环也可以理解为向量的操作，每一个样本点xi都对应了K种可能的分类yi，所以yi, F(xi), p(xi)都是一个K维的向量，这样或许容易理解一点）

 4. 表示求得残差减少的梯度方向

 5. 表示根据每一个样本点x，与其残差减少的梯度方向，得到一棵由J个叶子节点组成的决策树

 6. 为当决策树建立完成后，通过这个公式，可以得到每一个叶子节点的增益（这个增益在预测的时候用的）

       每个增益的组成其实也是一个K维的向量，表示如果在决策树预测的过程中，如果某一个样本点掉入了这个叶子节点，则其对应的K个分类的值是多少。比如说，GBDT得到了三棵决策树，一个样本点在预测的时候，也会掉入3个叶子节点上，其增益分别为（假设为3分类的问题）：

       (0.5, 0.8, 0.1),  (0.2, 0.6, 0.3),  (0.4, 0.3, 0.3)，那么这样最终得到的分类为第二个，因为选择分类2的决策树是最多的。

 7. 的意思为，将当前得到的决策树与之前的那些决策树合并起来，作为新的一个模型(跟6中所举的例子差不多)
 
 
<h2>参考</h2>
[机器学习中的算法(1)-决策树模型组合之随机森林与GBDT](http://www.cnblogs.com/LeftNotEasy/archive/2011/03/07/1976562.html)  
[梯度提升树(GBDT)原理小结](http://www.cnblogs.com/pinard/p/6140514.html)
  

