<h1>AdaBoost</h1>

AdaBoost属于Boosting方法。在提升(Boost)的方法中，权重赋予每个训练数据，迭代的学习k个分类器。学习得到分类器Mi之后，更新权重，使得其后的分类器Mi+1更关注Mi误分类的训练数据。最终提升的分类器M组合每个个体分类器的表决，其中每个分类器投票的权重是其准确率的函数。

<h2>介绍</h2>  

AdaBoost(Adaptive Boosting)是一种流行的提升算法。给定数据集D，包含d个类标记的数据，(X1,y1),(X2,y2)...(Xd,yd),其中yi是数据Xi的类标号。开始，AdaBoost对每个训练数据赋予相等的权重 1/d。为组合分类器产生k个基分类器需要执行算法的其余部分k轮。在第i轮中，从D中数据抽样，形成大小为d的训练集Di(使用有放回抽样，同一个数据可能被选中多次)。每个数据被选中的机会由它的权重决定。从训练集Di导出分类器Mi,然后使用Di作为检验集计算Mi的误差。训练数据的权重根据他们的分类情况调整。  
如果数据不正确的分类，则它的权重增加。反之减少。数据的权重反应对他们分类的困难程度－权重越高，越可能被错误的分类。然后，使用这些权重，为下一轮的分类器产生训练样本。  
基本思想：当建立分类器时候，希望更关注上一轮误分类的数据。某些分类器可能对某些困难数据分类效果比其他分类器要好。这样，建立了一个互补的分类器系列。

<h2>基本流程</h2>
算法的基本流程如图所示：  
![基本流程](http://images2015.cnblogs.com/blog/1042406/201612/1042406-20161204194331365-2142863547.png)  


 * 如何更新训练元素的权重？    

首先计算模型Mi的错误率=>Mi误分类Di中每个数据的加权和。也就是:

error(Mi) = Sum(Wi*err(Xj))
其中，wi为分类器的投票权重，err(Xj)是数据Xj的误分类误差：如果Xj被误分类，则err(Xj)为1，否则为0.如果err(Mi)>0,则丢弃。重新训练。  

第二，更新数据权重。如果数据在这轮被正确分类，则权重*err(Mi)/(1-error(Mi))，然后对所有数据的权重规范化。 这样就导致误分类数据的权重增加 。

 * 如何使用弱分类器得到强分类器？  
 
 首先计算每个分类器Mi的表决权重Wi :
 Wi = log((1-err(Mi))/err(Mi))
 这样，强分类器的结果为: Predict(Mi)*Wi所得分类概率之和，具有最大值的分类是最终的结果。  
 
 
<h2>总结</h2>

　这里对Adaboost算法的优缺点做一个总结。

　　　　Adaboost的主要优点有：

　　　　1）Adaboost作为分类器时，分类精度很高

　　　　2）在Adaboost的框架下，可以使用各种回归分类模型来构建弱学习器，非常灵活。

　　　　3）作为简单的二元分类器时，构造简单，结果可理解。

　　　　4）不容易发生过拟合

　　　　Adaboost的主要缺点有：

　　　　1）对异常样本敏感，异常样本在迭代中可能会获得较高的权重，影响最终的强学习器的预测准确性。


　　　　
<h2>参考</h2>
[集成学习之Adaboost算法原理小结](http://www.cnblogs.com/pinard/p/6133937.html)  
数据挖掘概念与技术 书籍





