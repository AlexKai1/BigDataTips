# 范数规则化

## L0, L1, L2
一般来说，监督学习可以看做最小化下面的目标函数：  
 
![](http://img.blog.csdn.net/20140504122253546?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvem91eHkwOQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)  

其中，第一项L(yi,f(xi;w)) 衡量我们的模型（分类或者回归）对第i个样本的预测值f(xi;w)和真实的标签yi之前的误差。也就是Loss函数。 Loss函数值越小，表示我们的训练得到的模型越拟合我们的训练数据。   
第二项是规则化函数Ω(w)。模型越复杂，则规范化的值越大。  
举个例子：  

![](http://img.blog.csdn.net/20140504122410234?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvem91eHkwOQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)  
从左到右分别是欠拟合（underfitting，也称High-bias）、合适的拟合和过拟合（overfitting，也称High variance）三种情况。  
如何防止出现overfitting的情况呢？  
咱们可以令:  f(x) =![](https://www.zhihu.com/equation?tex=w_%7B0%7Dx_%7B0%7D+%2B+w_%7B1%7D+x_%7B1%7D+%2B+w_%7B2%7Dx_%7B2%7D+%2B+w_%7B3%7Dx_%7B3%7D+%2B+.....+%2B+w_%7Bn%7Dx_%7Bn%7D) .ok，这个先介绍到这里，至于f(x)为什么用多项式的方式去模拟？相信也是很多人的疑问，很简单，大家看看高等数学当中的泰勒展开式就行了，任何函数都可以用多项式的方式去趋近，log x,lnx,等等都可以去趋近，而不同的函数曲线其实就是这些基础函数的组合，理所当然也可以用多项式去趋近。  
显而易见，我们从【过拟合】的图形可以看出f(x)的涉及到的特征项一定很多吧，即x_{0},x_{1},x_{2},x_{3}....x_{N}  等等很多吧？如何去防止过拟合，我们首先想到的就是控制N的数量吧，即让N最小化吧，而让N最小化，其实就是让W向量中项的个数最小化吧？
其中，W=(w_{0} ,w_{1} ,w_{2} ,w_{3} ,...w_{N} )
“让W向量中项的个数最小化”  = |W|0   
L0范数是指向量中非0的元素的个数。如果我们用L0范数来规则化一个参数矩阵W的话，就是希望W的大部分元素都是0。但是实际求解的难度是个NP完全问题。  
这个时候，L1范式出现了：  
![](https://pic1.zhimg.com/v2-6d969121bc4c9b71a9887276194cc4ac_b.png)  
L1范数是指向量中各个元素绝对值之和,也叫“稀疏规则算子”（Lasso regularization）L1范数是L0范数的最优凸近似，而且它比L0范数要容易优化求解。  
稀疏矩阵的优点：  

1）特征选择(Feature Selection)  
2）可解释性(Interpretability)

除了L1范数，还有一种更受宠幸的规则化范数是L2范数: ||W||2。在回归里面，有人把有它的回归叫“岭回归”（Ridge Regression），有人也叫它“权值衰减weight decay”.  
L2范数是指向量各元素的平方和然后求平方根。我们让L2范数的规则项||W||2最小，可以使得W的每个元素都很小，都接近于0，但与L1范数不同，它不会让它等于0，而是接近于0，这里是有很大的区别的哦。而越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象。  

## 参考资料  
[知乎讨论](https://www.zhihu.com/question/20924039)  
[机器学习中的范数规则化之（一）L0、L1与L2范数](http://blog.csdn.net/zouxy09/article/details/24971995)

