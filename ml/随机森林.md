<h1>Bagging,随机森林 </h1> 

<h2>介绍</h2>  

随机森林是集成学习的一种，集成学习有两个流派，一个是boosting派系，它的特点是各个弱学习器之间有依赖关系。另一种是bagging流派，它的特点是各个弱学习器之间没有依赖关系，可以并行拟合。本文就对集成学习中Bagging与随机森林算法做一个总结。

<h2>Bagging</h2>    

bagging学习原理：  
![学习原理](http://images2015.cnblogs.com/blog/1042406/201612/1042406-20161204200000787-1988863729.png)  
从上图可以看出，Bagging的弱学习器之间特点在“随机采样”。那么什么是随机采样？
  
随机采样(bootsrap)就是从我们的训练集里面采集固定个数的样本，但是每采集一个样本后，都将样本放回。也就是说，之前采集到的样本在放回后有可能继续被采集到。对于我们的Bagging算法，一般会随机采集和训练集样本数m一样个数的样本。这样得到的采样集和训练集样本的个数相同，但是样本内容不同。如果我们对有m个样本训练集做T次的随机采样，，则由于随机性，T个采样集各不相同。这里注意的是，Bagging的子采样是放回采样。  
bagging的集合策略也比较简单，对于分类问题，通常使用简单投票法，得到最多票数的类别或者类别之一为最终的模型输出。对于回归问题，通常使用简单平均法，对T个弱学习器得到的回归结果进行算术平均得到最终的模型输出。  
由于Bagging算法每次都进行采样来训练模型，因此泛化能力很强，对于降低模型的方差很有作用。当然对于训练集的拟合程度就会差一些，也就是模型的偏倚会大一些。

<h2>随机森林</h2>   
随机森林使用Bagging与随机属性选择来构建。
首先，RF使用了CART决策树作为弱学习器。第二，在使用决策树的基础上，RF对决策树的建立做了改进，对于普通的决策树，我们会在节点上所有的n个样本特征中选择一个最优的特征来做决策树的左右子树划分，但是RF通过随机选择节点上的一部分样本特征，这个数字小于n，假设为nsub，然后在这些随机选择的nsub个样本特征中，选择一个最优的特征来做决策树的左右子树划分。这样进一步增强了模型的泛化能力。第三，采样使用有放回的抽样。  
也就是说，随机森林每棵树的训练数据是bagging采样获得，属性是随机属性。  

特点：  

* 准确率高，同时不会overfitting 
* 可以并行处理 
 

<h2>参考</h2>

[Bagging与随机森林算法原理小结](http://www.cnblogs.com/pinard/p/6156009.html) ＝》 强烈推荐此人博客